{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "80000"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/ml-100k/u1.base') as file:\n",
    "    records = []\n",
    "    for line in file:\n",
    "        user, item, rating, _ = line.split('\t')\n",
    "\n",
    "        records.append([user, item, int(rating)])\n",
    "len(records)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "3.52835"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"global average rating\"\"\"\n",
    "ratings = []\n",
    "for record in records:\n",
    "    ratings.append(record[-1])\n",
    "r = sum(ratings) / len(ratings)\n",
    "r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"average rating of user u\"\"\"\n",
    "ratings_u, r_u = {}, {}\n",
    "for record in records:\n",
    "    user, item, rating = record\n",
    "    ratings_u.setdefault(user, {})\n",
    "    ratings_u[user][item] = rating\n",
    "\n",
    "for user in ratings_u.keys():\n",
    "    r_u[user] = sum(ratings_u[user].values()) / len(ratings_u[user].values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\"average rating of item i\"\"\"\n",
    "ratings_i, r_i = {}, {}\n",
    "for record in records:\n",
    "    user, item, rating = record\n",
    "    ratings_i.setdefault(item, {})\n",
    "    ratings_i[item][user] = rating\n",
    "\n",
    "for item in ratings_i.keys():\n",
    "    r_i[item] = sum(ratings_i[item].values()) / len(ratings_i[item].values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"bias of user u\"\"\"\n",
    "b_u = {}\n",
    "for user in ratings_u.keys():\n",
    "    b_u.setdefault(user, 0)\n",
    "    for item in ratings_u[user].keys():\n",
    "        b_u[user] += (ratings_u[user][item] - r_i[item]) / len(ratings_u[user].values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\"\"\"bias of item i\"\"\"\n",
    "b_i = {}\n",
    "for item in ratings_i.keys():\n",
    "    b_i.setdefault(item, 0)\n",
    "    for user in ratings_i[item].keys():\n",
    "        b_i[item] += (ratings_i[item][user] - r_u[user]) / len(ratings_i[item].values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk\n"
     ]
    },
    {
     "data": {
      "text/plain": "3.5359"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"load test data\"\"\"\n",
    "records_test = []\n",
    "users_test = []\n",
    "items_test = []\n",
    "ratings_test = []\n",
    "with open(\"data/ml-100k/u1.test\") as file:\n",
    "    for line in file:\n",
    "        user, item, rating, _ = line.split('\t')\n",
    "        if item == '599':\n",
    "            print(\"kk\")\n",
    "        records_test.append([user, item])\n",
    "        users_test.append(user)\n",
    "        items_test.append(item)\n",
    "        ratings_test.append(int(rating))\n",
    "users_test = np.array(users_test)\n",
    "items_test = np.array(items_test)\n",
    "ratings_test = np.array(ratings_test)\n",
    "np.average(ratings_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\"user average\"\"\"\n",
    "def user_avg():\n",
    "    ratings_predict = []\n",
    "    for user in users_test:\n",
    "        ratings_predict.append(r_u[user])\n",
    "    ratings_predict = np.array(ratings_predict)\n",
    "    # rmse_test = sqrt(metrics.mean_squared_error(ratings_test, ratings_predict))\n",
    "    # mae_test = metrics.mean_absolute_error(ratings_test, ratings_predict)\n",
    "    return ratings_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\"\"\"user average\"\"\"\n",
    "def item_avg():\n",
    "    ratings_predict = []\n",
    "    for item in items_test:\n",
    "        r_i.setdefault(item, r)\n",
    "        ratings_predict.append(r_i[item])\n",
    "    ratings_predict = np.array(ratings_predict)\n",
    "    # rmse_test = sqrt(metrics.mean_squared_error(ratings_test, ratings_predict))\n",
    "    # mae_test = metrics.mean_absolute_error(ratings_test, ratings_predict)\n",
    "    return ratings_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\"\"\"mean of user average and item average\"\"\"\n",
    "def mean_user_item():\n",
    "    \"\"\"return RMSE, MAE\"\"\"\n",
    "    ratings_predict = user_avg() / 2 + item_avg() / 2\n",
    "    rmse_test = sqrt(metrics.mean_squared_error(ratings_test, ratings_predict))\n",
    "    mae_test = metrics.mean_absolute_error(ratings_test, ratings_predict)\n",
    "    return rmse_test, mae_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"user bias and item average\"\"\"\n",
    "def user_bias_item_avg():\n",
    "    ratings_predict = []\n",
    "    for user, item in zip(users_test, items_test):\n",
    "        b_u.setdefault(user, 0)\n",
    "        r_i.setdefault(item, r)\n",
    "        ratings_predict.append(b_u[user] + r_i[item])\n",
    "    ratings_predict = np.array(ratings_predict)\n",
    "    # rmse_test = sqrt(metrics.mean_squared_error(ratings_test, ratings_predict))\n",
    "    # mae_test = metrics.mean_absolute_error(ratings_test, ratings_predict)\n",
    "    return ratings_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\"\"\"user average and item bias\"\"\"\n",
    "def user_avg_item_bias():\n",
    "    ratings_predict = []\n",
    "    for user, item in zip(users_test, items_test):\n",
    "        r_u.setdefault(user, r)\n",
    "        b_i.setdefault(item, 0)\n",
    "        ratings_predict.append(r_u[user] + b_i[item])\n",
    "    ratings_predict = np.array(ratings_predict)\n",
    "    # rmse_test = sqrt(metrics.mean_squared_error(ratings_test, ratings_predict))\n",
    "    # mae_test = metrics.mean_absolute_error(ratings_test, ratings_predict)\n",
    "    return ratings_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\"\"\"global average, user bias and item bias\"\"\"\n",
    "def global_avg_user_bias_item_bias():\n",
    "    ratings_predict = []\n",
    "    for user, item in zip(users_test, items_test):\n",
    "        b_u.setdefault(user, 0)\n",
    "        b_i.setdefault(item, 0)\n",
    "        ratings_predict.append(r + b_u[user] + b_i[item])\n",
    "    ratings_predict = np.array(ratings_predict)\n",
    "    # rmse_test = sqrt(metrics.mean_squared_error(ratings_test, ratings_predict))\n",
    "    # mae_test = metrics.mean_absolute_error(ratings_test, ratings_predict)\n",
    "    return ratings_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\"\"\"RMSE and MAE\"\"\"\n",
    "def performance(y_predict, y_test=ratings_test):\n",
    "    rmse_test = sqrt(metrics.mean_squared_error(y_test, y_predict))\n",
    "    mae_test = metrics.mean_absolute_error(y_test, y_predict)\n",
    "    return rmse_test, mae_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0629951276561334, 0.8501912740150434)\n",
      "(1.0334113714152895, 0.8275684032890005)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.962331641550567, 0.7612786028606267)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(performance(user_avg()))\n",
    "print(performance(item_avg()))\n",
    "mean_user_item()\n",
    "performance(user_bias_item_avg())\n",
    "performance(user_avg_item_bias())\n",
    "performance(global_avg_user_bias_item_bias())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 2], [1, 4]]\n"
     ]
    }
   ],
   "source": [
    "gg = [[1, 4], [3, 2]]\n",
    "print(sorted(gg, key=itemgetter(1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}